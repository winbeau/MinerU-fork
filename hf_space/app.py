"""
MinerU PDF è§£æå™¨ - HuggingFace Spaces ZeroGPU ç‰ˆæœ¬
ä¿®å¤ H200 MIG (slice) CUBLAS å…¼å®¹æ€§é—®é¢˜
"""

# ============================================
# å…³é”®ï¼šåœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
# ============================================
import os
import sys

# ç¦ç”¨å¤šè¿›ç¨‹
os.environ['MINERU_WORKER_NUM'] = '0'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# éšè—è­¦å‘Š
os.environ['ONNXRUNTIME_LOG_SEVERITY_LEVEL'] = '3'

# ç¦ç”¨ Flash Attentionï¼Œå¼ºåˆ¶ eager æ¨¡å¼
os.environ['ATTN_BACKEND'] = 'eager'
os.environ['TRANSFORMERS_ATTN_IMPLEMENTATION'] = 'eager'

# CUDA è®¾ç½®
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# ============================================
# Monkey-patch ProcessPoolExecutor
# ============================================
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor

class FakeProcessPoolExecutor(ThreadPoolExecutor):
    def __init__(self, max_workers=None, mp_context=None, initializer=None, initargs=()):
        super().__init__(max_workers=max_workers, initializer=initializer, initargs=initargs)

concurrent.futures.ProcessPoolExecutor = FakeProcessPoolExecutor

import multiprocessing
import multiprocessing.pool

class FakePool:
    def __init__(self, processes=None, initializer=None, initargs=(), maxtasksperchild=None, context=None):
        self._executor = ThreadPoolExecutor(max_workers=processes)
    def map(self, func, iterable, chunksize=None):
        return list(self._executor.map(func, iterable))
    def starmap(self, func, iterable, chunksize=None):
        return list(self._executor.map(lambda args: func(*args), iterable))
    def apply(self, func, args=(), kwds={}):
        return self._executor.submit(func, *args, **kwds).result()
    def apply_async(self, func, args=(), kwds={}, callback=None, error_callback=None):
        future = self._executor.submit(func, *args, **kwds)
        if callback:
            future.add_done_callback(lambda f: callback(f.result()))
        return future
    def close(self):
        self._executor.shutdown(wait=False)
    def terminate(self):
        self._executor.shutdown(wait=False, cancel_futures=True)
    def join(self):
        self._executor.shutdown(wait=True)
    def __enter__(self):
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.terminate()
        return False

multiprocessing.Pool = FakePool
multiprocessing.pool.Pool = FakePool

print("âœ… Monkey-patch: ProcessPoolExecutor â†’ ThreadPoolExecutor")

# ============================================
# Patch Tensor.__matmul__ (@ è¿ç®—ç¬¦) ä½¿ç”¨ CPU fallback
# ============================================
import torch

# ç¦ç”¨æ‰€æœ‰ SDPA ä¼˜åŒ–ï¼Œå¼ºåˆ¶ä½¿ç”¨ math å®ç°
if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
    torch.backends.cuda.enable_flash_sdp(False)
if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):
    torch.backends.cuda.enable_mem_efficient_sdp(False)
if hasattr(torch.backends.cuda, 'enable_math_sdp'):
    torch.backends.cuda.enable_math_sdp(True)

print("âœ… Disabled Flash/MemEfficient SDPA, using math SDPA only")

# ä¿å­˜åŸå§‹æ–¹æ³•
_original_tensor_matmul = torch.Tensor.__matmul__
_original_matmul = torch.matmul
_original_bmm = torch.bmm
_cublas_error_count = 0

def _safe_matmul_impl(a, b, original_fn):
    """é€šç”¨çš„å®‰å…¨çŸ©é˜µä¹˜æ³•å®ç°"""
    global _cublas_error_count
    try:
        return original_fn(a, b)
    except RuntimeError as e:
        if 'CUBLAS' in str(e):
            _cublas_error_count += 1
            if _cublas_error_count <= 5:
                print(f"âš ï¸ CUBLAS error #{_cublas_error_count}, falling back to CPU")
            # å›é€€åˆ° CPU
            device = a.device
            dtype = a.dtype
            result = original_fn(a.float().cpu(), b.float().cpu())
            return result.to(device=device, dtype=dtype)
        raise

def safe_tensor_matmul(self, other):
    """å®‰å…¨çš„ @ è¿ç®—ç¬¦"""
    return _safe_matmul_impl(self, other, _original_tensor_matmul)

def safe_matmul(input, other, *, out=None):
    """å®‰å…¨çš„ torch.matmul"""
    if out is not None:
        # æœ‰ out å‚æ•°æ—¶ä¸èƒ½ç®€å•å›é€€
        return _original_matmul(input, other, out=out)
    return _safe_matmul_impl(input, other, _original_matmul)

def safe_bmm(input, mat2, *, out=None):
    """å®‰å…¨çš„ torch.bmm"""
    if out is not None:
        return _original_bmm(input, mat2, out=out)
    return _safe_matmul_impl(input, mat2, _original_bmm)

# åº”ç”¨ patches
torch.Tensor.__matmul__ = safe_tensor_matmul
torch.matmul = safe_matmul
torch.bmm = safe_bmm

print("âœ… Monkey-patch: Tensor.__matmul__/matmul/bmm with CPU fallback")

# ============================================
# å¯¼å…¥å…¶ä»–æ¨¡å—
# ============================================
import spaces
import gradio as gr
import tempfile
import time
from pathlib import Path


@spaces.GPU(duration=300)
def parse_document(
    file,
    backend: str = "vlm-auto-engine",
    lang: str = "ch",
    max_pages: int = 5,
    table_enable: bool = True,
    formula_enable: bool = True,
):
    """GPU åŠ é€Ÿçš„æ–‡æ¡£è§£æå‡½æ•°"""
    import torch

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"âœ… GPU: {gpu_name} ({gpu_mem:.1f} GB)")

        # å†æ¬¡ç¡®ä¿ SDPA è®¾ç½®æ­£ç¡®
        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
            torch.backends.cuda.enable_flash_sdp(False)
            torch.backends.cuda.enable_mem_efficient_sdp(False)
    else:
        print("âŒ No GPU available!")
        return "é”™è¯¯ï¼šGPU ä¸å¯ç”¨", "", 0

    if file is None:
        return "è¯·ä¸Šä¼  PDF æˆ–å›¾ç‰‡æ–‡ä»¶", "", 0

    start_time = time.time()

    try:
        from mineru.cli.common import do_parse, read_fn
        from mineru.version import __version__

        with tempfile.TemporaryDirectory() as output_dir:
            file_path = Path(file.name if hasattr(file, 'name') else file)
            pdf_bytes = read_fn(file_path)
            file_stem = file_path.stem
            end_page = max_pages - 1 if max_pages else 99999

            os.environ['MINERU_VLM_FORMULA_ENABLE'] = str(formula_enable)
            os.environ['MINERU_VLM_TABLE_ENABLE'] = str(table_enable)

            print(f"ğŸ“„ å¼€å§‹è§£æ: {file_stem}")
            print(f"   Backend: {backend}, Language: {lang}, Max pages: {max_pages}")

            do_parse(
                output_dir=output_dir,
                pdf_file_names=[file_stem],
                pdf_bytes_list=[pdf_bytes],
                p_lang_list=[lang],
                backend=backend,
                parse_method="auto",
                formula_enable=formula_enable,
                table_enable=table_enable,
                f_draw_layout_bbox=False,
                f_draw_span_bbox=False,
                f_dump_md=True,
                f_dump_middle_json=False,
                f_dump_model_output=False,
                f_dump_orig_pdf=False,
                f_dump_content_list=False,
                start_page_id=0,
                end_page_id=end_page,
            )

            # ç¡®å®šç»“æœè·¯å¾„
            if backend == "pipeline":
                result_dir = os.path.join(output_dir, file_stem, "auto")
            elif backend.startswith("vlm"):
                result_dir = os.path.join(output_dir, file_stem, "vlm")
            else:
                result_dir = os.path.join(output_dir, file_stem, "hybrid_auto")

            md_path = os.path.join(result_dir, f"{file_stem}.md")
            elapsed = time.time() - start_time

            if os.path.exists(md_path):
                with open(md_path, "r", encoding="utf-8") as f:
                    markdown = f.read()
                status = f"âœ… è§£ææˆåŠŸï¼è€—æ—¶ {elapsed:.1f} ç§’ (MinerU v{__version__}, GPU: {gpu_name})"
                print(status)
                return status, markdown, elapsed
            else:
                for root, dirs, files in os.walk(output_dir):
                    for f in files:
                        if f.endswith('.md'):
                            with open(os.path.join(root, f), "r", encoding="utf-8") as file:
                                markdown = file.read()
                            return f"âœ… è§£ææˆåŠŸï¼è€—æ—¶ {elapsed:.1f} ç§’", markdown, elapsed
                return f"âŒ è§£æå¤±è´¥ï¼šæœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶", "", elapsed

    except Exception as e:
        elapsed = time.time() - start_time
        error_msg = f"âŒ è§£æé”™è¯¯: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, "", elapsed


# Gradio ç•Œé¢
with gr.Blocks(title="MinerU PDF è§£æå™¨ (ZeroGPU)", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ“„ MinerU PDF è§£æå™¨
    ### ğŸš€ Powered by HuggingFace ZeroGPU (H200 Slice)

    å°† PDF/å›¾ç‰‡è½¬æ¢ä¸º Markdownï¼Œæ”¯æŒè¡¨æ ¼ã€å…¬å¼è¯†åˆ«ã€‚
    """)

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(
                label="ä¸Šä¼ æ–‡ä»¶",
                file_types=[".pdf", ".png", ".jpg", ".jpeg", ".webp", ".gif", ".bmp", ".tiff"],
            )

            backend = gr.Dropdown(
                choices=[
                    ("VLM æ¨¡å¼ (æ¨è)", "vlm-auto-engine"),
                    ("æ··åˆæ¨¡å¼", "hybrid-auto-engine"),
                    ("Pipeline æ¨¡å¼", "pipeline"),
                ],
                value="vlm-auto-engine",
                label="è§£æåç«¯",
            )

            lang = gr.Dropdown(
                choices=[
                    ("ä¸­æ–‡", "ch"),
                    ("è‹±æ–‡", "en"),
                    ("è‡ªåŠ¨æ£€æµ‹", "auto"),
                ],
                value="ch",
                label="æ–‡æ¡£è¯­è¨€",
            )

            max_pages = gr.Slider(minimum=1, maximum=20, value=3, step=1, label="æœ€å¤§é¡µæ•°")

            with gr.Row():
                table_enable = gr.Checkbox(value=True, label="è¡¨æ ¼è¯†åˆ«")
                formula_enable = gr.Checkbox(value=True, label="å…¬å¼è¯†åˆ«")

            btn = gr.Button("ğŸš€ å¼€å§‹è§£æ", variant="primary", size="lg")

        with gr.Column(scale=2):
            status = gr.Textbox(label="çŠ¶æ€", interactive=False)
            elapsed = gr.Number(label="è€—æ—¶ (ç§’)", interactive=False)
            output = gr.Markdown(label="è§£æç»“æœ")

    btn.click(
        fn=parse_document,
        inputs=[file_input, backend, lang, max_pages, table_enable, formula_enable],
        outputs=[status, output, elapsed],
    )

    gr.Markdown("""
    ---
    ### âš ï¸ è¯´æ˜
    - H200 MIG åˆ†åŒºå¯èƒ½å­˜åœ¨ CUBLAS å…¼å®¹æ€§é—®é¢˜
    - å¦‚æœè§£æå¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ° CPU è®¡ç®—ï¼ˆè¾ƒæ…¢ä½†ç¨³å®šï¼‰
    - å»ºè®®å…ˆç”¨ 1-3 é¡µæµ‹è¯•
    """)

if __name__ == "__main__":
    demo.launch()
